{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"./preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry_id</th>\n",
       "      <th>len_description</th>\n",
       "      <th>len_html2text</th>\n",
       "      <th>description</th>\n",
       "      <th>html2text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>3254</td>\n",
       "      <td>webhostcom offers budget and unlimited web hos...</td>\n",
       "      <td>reliable web hosting services from webhosth fo...</td>\n",
       "      <td>webhostcom offers budget and unlimited web hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>323</td>\n",
       "      <td>18637</td>\n",
       "      <td>we are a direct cash advance provider with fun...</td>\n",
       "      <td>abc merchant funding advanced business capital...</td>\n",
       "      <td>we are a direct cash advance provider with fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>11663</td>\n",
       "      <td>able investigation enforcements are an establi...</td>\n",
       "      <td>able investigations bristol based enforcement ...</td>\n",
       "      <td>able investigation enforcements are an establi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1125</td>\n",
       "      <td>1067</td>\n",
       "      <td>for over two decades abm has been known for it...</td>\n",
       "      <td>abm group of companyhome site map client login...</td>\n",
       "      <td>for over two decades abm has been known for it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>12</td>\n",
       "      <td>additionally lets you easily create the best p...</td>\n",
       "      <td>additionally</td>\n",
       "      <td>additionally lets you easily create the best p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   industry_id  len_description  len_html2text  \\\n",
       "0            0              598           3254   \n",
       "1            1              323          18637   \n",
       "2            2              681          11663   \n",
       "3            3             1125           1067   \n",
       "4            0              160             12   \n",
       "\n",
       "                                         description  \\\n",
       "0  webhostcom offers budget and unlimited web hos...   \n",
       "1  we are a direct cash advance provider with fun...   \n",
       "2  able investigation enforcements are an establi...   \n",
       "3  for over two decades abm has been known for it...   \n",
       "4  additionally lets you easily create the best p...   \n",
       "\n",
       "                                           html2text  \\\n",
       "0  reliable web hosting services from webhosth fo...   \n",
       "1  abc merchant funding advanced business capital...   \n",
       "2  able investigations bristol based enforcement ...   \n",
       "3  abm group of companyhome site map client login...   \n",
       "4                                       additionally   \n",
       "\n",
       "                                                text  \n",
       "0  webhostcom offers budget and unlimited web hos...  \n",
       "1  we are a direct cash advance provider with fun...  \n",
       "2  able investigation enforcements are an establi...  \n",
       "3  for over two decades abm has been known for it...  \n",
       "4  additionally lets you easily create the best p...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['description'].astype(str) + ' ' + df['html2text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=32)\n",
    "for train_index, test_index in sss.split(df['text'], df[\"industry_id\"]):\n",
    "    X_train, X_test = df['text'][train_index], df['text'][test_index]\n",
    "    y_train, y_test = df[\"industry_id\"][train_index], df[\"industry_id\"][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "import Stemmer\n",
    "english_stemmer = Stemmer.Stemmer('en')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: english_stemmer.stemWords(analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model_description = Pipeline([\n",
    "    ('vect', StemmedTfidfVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "    ('clf', LinearSVC(C=0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "            ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_description.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model_description.predict(X_train)\n",
    "pred_test = model_description.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1119,    0,    0, ...,    0,    0,    1],\n",
       "       [   2,  819,    0, ...,    1,    0,    0],\n",
       "       [   0,    0,  274, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   0,    0,    1, ...,  191,    0,    0],\n",
       "       [   0,    0,    1, ...,    0,  176,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,  151]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972256994958893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 81,   3,   0, ...,   0,   0,   1],\n",
       "       [  6, 154,   2, ...,   0,   1,   0],\n",
       "       [  0,   0,  19, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  18,   0,   0],\n",
       "       [  1,   0,   1, ...,   0,  26,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,  15]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506765899864682"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.96      0.97      1165\n",
      "          1       0.97      0.97      0.97       844\n",
      "          2       1.00      0.93      0.96       295\n",
      "          3       0.94      0.97      0.96      2329\n",
      "          4       1.00      0.97      0.98       424\n",
      "          5       0.98      0.98      0.98       395\n",
      "          6       0.99      0.96      0.98       441\n",
      "          7       0.82      0.97      0.89       298\n",
      "          8       0.98      0.96      0.97       461\n",
      "          9       0.98      0.98      0.98       262\n",
      "         10       0.99      0.97      0.98       205\n",
      "         11       0.95      0.99      0.97      1098\n",
      "         12       0.95      0.99      0.97       464\n",
      "         13       0.98      0.98      0.98       442\n",
      "         14       0.99      0.98      0.99       586\n",
      "         15       0.98      0.94      0.96       756\n",
      "         16       0.93      0.98      0.96      1932\n",
      "         17       0.99      0.96      0.98       311\n",
      "         18       0.99      0.94      0.97       174\n",
      "         19       0.99      0.99      0.99       286\n",
      "         20       0.99      0.97      0.98       473\n",
      "         21       0.98      0.98      0.98       253\n",
      "         22       0.97      0.99      0.98       944\n",
      "         23       0.98      0.97      0.97      1072\n",
      "         24       1.00      0.96      0.98       211\n",
      "         25       0.99      0.98      0.99       719\n",
      "         26       0.99      0.97      0.98       326\n",
      "         27       1.00      0.98      0.99       192\n",
      "         28       1.00      0.96      0.98       228\n",
      "         29       0.98      0.97      0.98       663\n",
      "         30       0.99      0.93      0.96       164\n",
      "         31       0.97      0.98      0.98       508\n",
      "         32       1.00      0.89      0.94       269\n",
      "         33       0.97      0.97      0.97       592\n",
      "         34       0.99      0.97      0.98       426\n",
      "         35       1.00      0.96      0.98       437\n",
      "         36       0.97      0.99      0.98       524\n",
      "         37       0.99      0.97      0.98       381\n",
      "         38       0.99      0.99      0.99       278\n",
      "         39       0.99      0.99      0.99       324\n",
      "         40       1.00      0.98      0.99       200\n",
      "         41       0.97      0.98      0.98       410\n",
      "         42       0.98      0.97      0.98       435\n",
      "         43       0.95      0.97      0.96       667\n",
      "         44       0.99      0.97      0.98       176\n",
      "         45       1.00      0.96      0.98       368\n",
      "         46       1.00      0.99      1.00       169\n",
      "         47       1.00      0.99      1.00       180\n",
      "         48       0.97      0.99      0.98       697\n",
      "         49       0.87      0.95      0.91       202\n",
      "         50       0.99      0.98      0.99       437\n",
      "         51       1.00      0.97      0.99       398\n",
      "         52       0.96      0.97      0.97       293\n",
      "         53       1.00      0.98      0.99       242\n",
      "         54       1.00      0.98      0.99       262\n",
      "         55       1.00      0.97      0.99       211\n",
      "         56       0.98      0.98      0.98       383\n",
      "         57       0.98      0.98      0.98       215\n",
      "         58       1.00      0.85      0.92       171\n",
      "         59       0.98      0.96      0.97       291\n",
      "         60       0.99      0.99      0.99       247\n",
      "         61       0.96      0.94      0.95       219\n",
      "         62       0.99      0.98      0.98       195\n",
      "         63       0.97      0.99      0.98       178\n",
      "         64       1.00      0.95      0.97       159\n",
      "\n",
      "avg / total       0.97      0.97      0.97     29557\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.28      0.29       292\n",
      "          1       0.69      0.73      0.71       211\n",
      "          2       0.51      0.26      0.34        74\n",
      "          3       0.40      0.63      0.49       582\n",
      "          4       0.55      0.40      0.46       106\n",
      "          5       0.61      0.66      0.63        99\n",
      "          6       0.52      0.40      0.45       110\n",
      "          7       0.51      0.54      0.53        74\n",
      "          8       0.75      0.65      0.70       115\n",
      "          9       0.71      0.82      0.76        65\n",
      "         10       0.43      0.18      0.25        51\n",
      "         11       0.50      0.70      0.58       275\n",
      "         12       0.69      0.81      0.74       116\n",
      "         13       0.53      0.55      0.54       110\n",
      "         14       0.39      0.40      0.40       146\n",
      "         15       0.39      0.22      0.28       189\n",
      "         16       0.45      0.67      0.54       483\n",
      "         17       0.55      0.47      0.51        78\n",
      "         18       0.00      0.00      0.00        43\n",
      "         19       0.49      0.39      0.44        71\n",
      "         20       0.42      0.42      0.42       118\n",
      "         21       0.57      0.75      0.64        63\n",
      "         22       0.72      0.81      0.77       236\n",
      "         23       0.45      0.53      0.48       268\n",
      "         24       0.45      0.36      0.40        53\n",
      "         25       0.53      0.52      0.53       180\n",
      "         26       0.48      0.37      0.42        82\n",
      "         27       0.61      0.56      0.59        48\n",
      "         28       0.66      0.37      0.47        57\n",
      "         29       0.36      0.30      0.32       166\n",
      "         30       0.55      0.15      0.23        41\n",
      "         31       0.56      0.65      0.60       127\n",
      "         32       0.45      0.22      0.30        67\n",
      "         33       0.49      0.56      0.52       148\n",
      "         34       0.63      0.48      0.54       107\n",
      "         35       0.54      0.39      0.46       109\n",
      "         36       0.70      0.68      0.69       131\n",
      "         37       0.59      0.55      0.57        95\n",
      "         38       0.52      0.38      0.44        69\n",
      "         39       0.48      0.53      0.50        81\n",
      "         40       0.48      0.22      0.30        50\n",
      "         41       0.81      0.72      0.76       103\n",
      "         42       0.63      0.58      0.60       109\n",
      "         43       0.51      0.57      0.54       167\n",
      "         44       0.59      0.23      0.33        44\n",
      "         45       0.51      0.36      0.42        92\n",
      "         46       0.58      0.36      0.44        42\n",
      "         47       0.52      0.36      0.42        45\n",
      "         48       0.47      0.57      0.51       175\n",
      "         49       0.46      0.50      0.48        50\n",
      "         50       0.68      0.65      0.66       109\n",
      "         51       0.36      0.31      0.33        99\n",
      "         52       0.51      0.48      0.50        73\n",
      "         53       0.38      0.18      0.25        60\n",
      "         54       0.41      0.14      0.20        66\n",
      "         55       0.50      0.09      0.16        53\n",
      "         56       0.59      0.53      0.56        96\n",
      "         57       0.57      0.54      0.55        54\n",
      "         58       0.25      0.05      0.08        43\n",
      "         59       0.30      0.15      0.20        73\n",
      "         60       0.52      0.53      0.53        62\n",
      "         61       0.38      0.20      0.26        55\n",
      "         62       0.55      0.37      0.44        49\n",
      "         63       0.70      0.58      0.63        45\n",
      "         64       0.52      0.38      0.43        40\n",
      "\n",
      "avg / total       0.51      0.51      0.49      7390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_train, pred_train))\n",
    "print(metrics.classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
