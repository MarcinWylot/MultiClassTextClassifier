{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"./preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>industry_id</th>\n",
       "      <th>len_description</th>\n",
       "      <th>len_html2text</th>\n",
       "      <th>description</th>\n",
       "      <th>html2text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>598</td>\n",
       "      <td>3254</td>\n",
       "      <td>webhostcom offers budget and unlimited web hos...</td>\n",
       "      <td>reliable web hosting services from webhosth fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>323</td>\n",
       "      <td>18637</td>\n",
       "      <td>we are a direct cash advance provider with fun...</td>\n",
       "      <td>abc merchant funding advanced business capital...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>11663</td>\n",
       "      <td>able investigation enforcements are an establi...</td>\n",
       "      <td>able investigations bristol based enforcement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1125</td>\n",
       "      <td>1067</td>\n",
       "      <td>for over two decades abm has been known for it...</td>\n",
       "      <td>abm group of companyhome site map client login...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>12</td>\n",
       "      <td>additionally lets you easily create the best p...</td>\n",
       "      <td>additionally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   industry_id  len_description  len_html2text  \\\n",
       "0            0              598           3254   \n",
       "1            1              323          18637   \n",
       "2            2              681          11663   \n",
       "3            3             1125           1067   \n",
       "4            0              160             12   \n",
       "\n",
       "                                         description  \\\n",
       "0  webhostcom offers budget and unlimited web hos...   \n",
       "1  we are a direct cash advance provider with fun...   \n",
       "2  able investigation enforcements are an establi...   \n",
       "3  for over two decades abm has been known for it...   \n",
       "4  additionally lets you easily create the best p...   \n",
       "\n",
       "                                           html2text  \n",
       "0  reliable web hosting services from webhosth fo...  \n",
       "1  abc merchant funding advanced business capital...  \n",
       "2  able investigations bristol based enforcement ...  \n",
       "3  abm group of companyhome site map client login...  \n",
       "4                                       additionally  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=32)\n",
    "for train_index, test_index in sss.split(df, df[\"industry_id\"]):\n",
    "    X_train, X_test = df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train, y_test = df[\"industry_id\"][train_index], df[\"industry_id\"][test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "import Stemmer\n",
    "english_stemmer = Stemmer.Stemmer('en')\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: english_stemmer.stemWords(analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model_description = Pipeline([\n",
    "    ('vect', StemmedTfidfVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "    ('clf', LinearSVC(C=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "            ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_description.fit(X_train['description'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_html2text = Pipeline([\n",
    "    ('vect', StemmedTfidfVectorizer(stop_words='english', ngram_range=(1,1))),\n",
    "    ('clf', LinearSVC(C=2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "            ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_html2text.fit(X_train['html2text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stacked = pd.DataFrame()\n",
    "X_train_stacked['len_description'] = X_train['len_description']\n",
    "X_train_stacked['len_html2text'] = X_train['len_html2text']\n",
    "X_train_stacked['pred_description'] = model_description.predict(X_train['description'])\n",
    "X_train_stacked['pred_html2text'] = model_html2text.predict(X_train['html2text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model_stacked = XGBClassifier(max_depth=3, n_estimators=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=50,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_stacked.fit(X_train_stacked, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\martinX1\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "pred_train = model_stacked.predict(X_train_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1165,    0,    0, ...,    0,    0,    0],\n",
       "       [   0,  844,    0, ...,    0,    0,    0],\n",
       "       [   0,    0,  295, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  195,    0,    0],\n",
       "       [   0,    0,    0, ...,    0,  178,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,  159]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977331934905437"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      1165\n",
      "          1       1.00      1.00      1.00       844\n",
      "          2       0.99      1.00      1.00       295\n",
      "          3       1.00      0.99      1.00      2329\n",
      "          4       1.00      1.00      1.00       424\n",
      "          5       1.00      1.00      1.00       395\n",
      "          6       1.00      1.00      1.00       441\n",
      "          7       1.00      1.00      1.00       298\n",
      "          8       1.00      1.00      1.00       461\n",
      "          9       1.00      1.00      1.00       262\n",
      "         10       1.00      1.00      1.00       205\n",
      "         11       1.00      1.00      1.00      1098\n",
      "         12       1.00      1.00      1.00       464\n",
      "         13       1.00      1.00      1.00       442\n",
      "         14       1.00      1.00      1.00       586\n",
      "         15       1.00      1.00      1.00       756\n",
      "         16       1.00      0.99      0.99      1932\n",
      "         17       1.00      1.00      1.00       311\n",
      "         18       1.00      1.00      1.00       174\n",
      "         19       0.99      1.00      1.00       286\n",
      "         20       1.00      0.99      1.00       473\n",
      "         21       1.00      1.00      1.00       253\n",
      "         22       1.00      1.00      1.00       944\n",
      "         23       1.00      1.00      1.00      1072\n",
      "         24       1.00      1.00      1.00       211\n",
      "         25       1.00      1.00      1.00       719\n",
      "         26       1.00      1.00      1.00       326\n",
      "         27       0.99      1.00      1.00       192\n",
      "         28       1.00      1.00      1.00       228\n",
      "         29       1.00      1.00      1.00       663\n",
      "         30       1.00      1.00      1.00       164\n",
      "         31       1.00      1.00      1.00       508\n",
      "         32       1.00      1.00      1.00       269\n",
      "         33       1.00      1.00      1.00       592\n",
      "         34       1.00      1.00      1.00       426\n",
      "         35       1.00      1.00      1.00       437\n",
      "         36       1.00      1.00      1.00       524\n",
      "         37       1.00      1.00      1.00       381\n",
      "         38       1.00      1.00      1.00       278\n",
      "         39       1.00      1.00      1.00       324\n",
      "         40       1.00      1.00      1.00       200\n",
      "         41       1.00      1.00      1.00       410\n",
      "         42       1.00      1.00      1.00       435\n",
      "         43       1.00      1.00      1.00       667\n",
      "         44       1.00      0.99      1.00       176\n",
      "         45       1.00      1.00      1.00       368\n",
      "         46       1.00      1.00      1.00       169\n",
      "         47       1.00      0.99      1.00       180\n",
      "         48       1.00      1.00      1.00       697\n",
      "         49       1.00      1.00      1.00       202\n",
      "         50       1.00      1.00      1.00       437\n",
      "         51       1.00      1.00      1.00       398\n",
      "         52       1.00      1.00      1.00       293\n",
      "         53       1.00      1.00      1.00       242\n",
      "         54       1.00      1.00      1.00       262\n",
      "         55       0.99      1.00      1.00       211\n",
      "         56       0.99      1.00      1.00       383\n",
      "         57       1.00      1.00      1.00       215\n",
      "         58       1.00      0.99      1.00       171\n",
      "         59       1.00      0.99      1.00       291\n",
      "         60       1.00      1.00      1.00       247\n",
      "         61       1.00      1.00      1.00       219\n",
      "         62       1.00      1.00      1.00       195\n",
      "         63       0.99      1.00      1.00       178\n",
      "         64       1.00      1.00      1.00       159\n",
      "\n",
      "avg / total       1.00      1.00      1.00     29557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\martinX1\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "X_test_stacked = pd.DataFrame()\n",
    "X_test_stacked['len_description'] = X_test['len_description']\n",
    "X_test_stacked['len_html2text'] = X_test['len_html2text']\n",
    "X_test_stacked['pred_description'] = model_description.predict(X_test['description'])\n",
    "X_test_stacked['pred_html2text'] = model_html2text.predict(X_test['html2text'])\n",
    "pred_test = model_stacked.predict(X_test_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 91,   3,   1, ...,   0,   0,   1],\n",
       "       [  5, 150,   2, ...,   0,   1,   0],\n",
       "       [  0,   0,  20, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   1,   0, ...,  20,   1,   0],\n",
       "       [  0,   0,   1, ...,   0,  30,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,  19]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5259810554803789"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.31      0.30       292\n",
      "          1       0.64      0.71      0.67       211\n",
      "          2       0.50      0.27      0.35        74\n",
      "          3       0.46      0.57      0.51       582\n",
      "          4       0.54      0.49      0.51       106\n",
      "          5       0.58      0.65      0.61        99\n",
      "          6       0.46      0.42      0.44       110\n",
      "          7       0.60      0.50      0.54        74\n",
      "          8       0.73      0.67      0.70       115\n",
      "          9       0.71      0.75      0.73        65\n",
      "         10       0.52      0.29      0.38        51\n",
      "         11       0.56      0.63      0.59       275\n",
      "         12       0.69      0.81      0.75       116\n",
      "         13       0.58      0.59      0.58       110\n",
      "         14       0.43      0.47      0.45       146\n",
      "         15       0.34      0.26      0.29       189\n",
      "         16       0.52      0.59      0.55       483\n",
      "         17       0.52      0.47      0.50        78\n",
      "         18       0.33      0.12      0.17        43\n",
      "         19       0.53      0.44      0.48        71\n",
      "         20       0.43      0.39      0.41       118\n",
      "         21       0.60      0.81      0.69        63\n",
      "         22       0.76      0.78      0.77       236\n",
      "         23       0.44      0.49      0.46       268\n",
      "         24       0.64      0.53      0.58        53\n",
      "         25       0.58      0.54      0.56       180\n",
      "         26       0.42      0.39      0.41        82\n",
      "         27       0.60      0.50      0.55        48\n",
      "         28       0.60      0.44      0.51        57\n",
      "         29       0.38      0.33      0.35       166\n",
      "         30       0.58      0.27      0.37        41\n",
      "         31       0.55      0.64      0.59       127\n",
      "         32       0.37      0.27      0.31        67\n",
      "         33       0.57      0.66      0.61       148\n",
      "         34       0.62      0.60      0.61       107\n",
      "         35       0.48      0.44      0.46       109\n",
      "         36       0.73      0.78      0.75       131\n",
      "         37       0.57      0.51      0.54        95\n",
      "         38       0.47      0.33      0.39        69\n",
      "         39       0.61      0.63      0.62        81\n",
      "         40       0.68      0.46      0.55        50\n",
      "         41       0.72      0.74      0.73       103\n",
      "         42       0.63      0.62      0.63       109\n",
      "         43       0.55      0.65      0.60       167\n",
      "         44       0.48      0.32      0.38        44\n",
      "         45       0.44      0.42      0.43        92\n",
      "         46       0.54      0.52      0.53        42\n",
      "         47       0.64      0.51      0.57        45\n",
      "         48       0.47      0.53      0.50       175\n",
      "         49       0.59      0.46      0.52        50\n",
      "         50       0.56      0.73      0.63       109\n",
      "         51       0.39      0.39      0.39        99\n",
      "         52       0.56      0.51      0.53        73\n",
      "         53       0.37      0.30      0.33        60\n",
      "         54       0.33      0.26      0.29        66\n",
      "         55       0.28      0.15      0.20        53\n",
      "         56       0.64      0.57      0.60        96\n",
      "         57       0.69      0.67      0.68        54\n",
      "         58       0.50      0.16      0.25        43\n",
      "         59       0.42      0.34      0.38        73\n",
      "         60       0.48      0.48      0.48        62\n",
      "         61       0.40      0.38      0.39        55\n",
      "         62       0.51      0.41      0.45        49\n",
      "         63       0.71      0.67      0.69        45\n",
      "         64       0.63      0.47      0.54        40\n",
      "\n",
      "avg / total       0.52      0.53      0.52      7390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      1165\n",
      "          1       1.00      0.99      0.99       844\n",
      "          2       0.99      1.00      0.99       295\n",
      "          3       0.96      0.99      0.97      2329\n",
      "          4       1.00      1.00      1.00       424\n",
      "          5       0.99      0.99      0.99       395\n",
      "          6       1.00      0.99      1.00       441\n",
      "          7       1.00      1.00      1.00       298\n",
      "          8       1.00      0.99      0.99       461\n",
      "          9       0.98      0.99      0.99       262\n",
      "         10       1.00      1.00      1.00       205\n",
      "         11       0.98      0.99      0.99      1098\n",
      "         12       1.00      0.99      1.00       464\n",
      "         13       1.00      1.00      1.00       442\n",
      "         14       1.00      0.99      1.00       586\n",
      "         15       1.00      0.99      1.00       756\n",
      "         16       0.99      0.99      0.99      1932\n",
      "         17       1.00      0.99      0.99       311\n",
      "         18       1.00      0.99      1.00       174\n",
      "         19       0.99      1.00      1.00       286\n",
      "         20       1.00      0.99      0.99       473\n",
      "         21       1.00      0.99      1.00       253\n",
      "         22       0.99      1.00      1.00       944\n",
      "         23       0.99      0.99      0.99      1072\n",
      "         24       1.00      1.00      1.00       211\n",
      "         25       1.00      1.00      1.00       719\n",
      "         26       1.00      1.00      1.00       326\n",
      "         27       0.99      1.00      1.00       192\n",
      "         28       1.00      1.00      1.00       228\n",
      "         29       0.99      0.99      0.99       663\n",
      "         30       1.00      1.00      1.00       164\n",
      "         31       1.00      0.99      1.00       508\n",
      "         32       1.00      1.00      1.00       269\n",
      "         33       1.00      1.00      1.00       592\n",
      "         34       1.00      1.00      1.00       426\n",
      "         35       1.00      0.98      0.99       437\n",
      "         36       1.00      0.99      1.00       524\n",
      "         37       1.00      0.99      0.99       381\n",
      "         38       0.99      1.00      1.00       278\n",
      "         39       1.00      1.00      1.00       324\n",
      "         40       1.00      1.00      1.00       200\n",
      "         41       1.00      1.00      1.00       410\n",
      "         42       1.00      0.99      0.99       435\n",
      "         43       1.00      0.99      0.99       667\n",
      "         44       1.00      0.99      1.00       176\n",
      "         45       1.00      0.99      0.99       368\n",
      "         46       1.00      1.00      1.00       169\n",
      "         47       1.00      0.99      1.00       180\n",
      "         48       1.00      1.00      1.00       697\n",
      "         49       1.00      1.00      1.00       202\n",
      "         50       1.00      0.98      0.99       437\n",
      "         51       1.00      0.98      0.99       398\n",
      "         52       1.00      1.00      1.00       293\n",
      "         53       1.00      0.99      1.00       242\n",
      "         54       1.00      0.98      0.99       262\n",
      "         55       0.99      0.98      0.99       211\n",
      "         56       1.00      0.99      1.00       383\n",
      "         57       1.00      0.99      0.99       215\n",
      "         58       1.00      0.99      1.00       171\n",
      "         59       1.00      0.99      1.00       291\n",
      "         60       1.00      1.00      1.00       247\n",
      "         61       1.00      1.00      1.00       219\n",
      "         62       1.00      0.98      0.99       195\n",
      "         63       0.99      1.00      1.00       178\n",
      "         64       1.00      1.00      1.00       159\n",
      "\n",
      "avg / total       0.99      0.99      0.99     29557\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.94      0.95      1165\n",
      "          1       0.99      0.91      0.95       844\n",
      "          2       0.99      0.91      0.94       295\n",
      "          3       0.71      0.97      0.82      2329\n",
      "          4       0.98      0.91      0.94       424\n",
      "          5       1.00      0.95      0.97       395\n",
      "          6       1.00      0.91      0.95       441\n",
      "          7       0.77      0.97      0.86       298\n",
      "          8       1.00      0.90      0.95       461\n",
      "          9       1.00      0.92      0.96       262\n",
      "         10       0.99      0.94      0.97       205\n",
      "         11       0.98      0.93      0.96      1098\n",
      "         12       0.96      0.93      0.94       464\n",
      "         13       0.96      0.91      0.94       442\n",
      "         14       1.00      0.95      0.98       586\n",
      "         15       0.97      0.92      0.94       756\n",
      "         16       0.83      0.96      0.89      1932\n",
      "         17       0.99      0.90      0.94       311\n",
      "         18       0.99      0.96      0.97       174\n",
      "         19       0.99      0.94      0.96       286\n",
      "         20       0.98      0.93      0.95       473\n",
      "         21       1.00      0.89      0.94       253\n",
      "         22       0.99      0.94      0.96       944\n",
      "         23       0.94      0.92      0.93      1072\n",
      "         24       1.00      0.95      0.98       211\n",
      "         25       0.93      0.94      0.93       719\n",
      "         26       1.00      0.90      0.95       326\n",
      "         27       0.99      0.93      0.96       192\n",
      "         28       1.00      0.89      0.94       228\n",
      "         29       0.96      0.91      0.94       663\n",
      "         30       0.97      0.89      0.93       164\n",
      "         31       0.99      0.94      0.96       508\n",
      "         32       1.00      0.91      0.95       269\n",
      "         33       0.89      0.94      0.92       592\n",
      "         34       1.00      0.94      0.97       426\n",
      "         35       0.96      0.94      0.95       437\n",
      "         36       0.99      0.95      0.97       524\n",
      "         37       1.00      0.95      0.97       381\n",
      "         38       1.00      0.95      0.97       278\n",
      "         39       0.90      0.91      0.91       324\n",
      "         40       1.00      0.91      0.95       200\n",
      "         41       0.99      0.93      0.96       410\n",
      "         42       1.00      0.94      0.97       435\n",
      "         43       0.97      0.92      0.94       667\n",
      "         44       1.00      0.93      0.96       176\n",
      "         45       1.00      0.93      0.96       368\n",
      "         46       1.00      0.93      0.96       169\n",
      "         47       0.99      0.96      0.97       180\n",
      "         48       0.99      0.96      0.97       697\n",
      "         49       0.83      0.92      0.88       202\n",
      "         50       1.00      0.95      0.97       437\n",
      "         51       0.99      0.93      0.96       398\n",
      "         52       0.99      0.92      0.95       293\n",
      "         53       1.00      0.97      0.99       242\n",
      "         54       1.00      0.92      0.96       262\n",
      "         55       1.00      0.92      0.96       211\n",
      "         56       0.99      0.92      0.96       383\n",
      "         57       0.99      0.94      0.96       215\n",
      "         58       0.99      0.91      0.95       171\n",
      "         59       0.98      0.95      0.96       291\n",
      "         60       0.98      0.95      0.97       247\n",
      "         61       0.96      0.89      0.92       219\n",
      "         62       1.00      0.92      0.96       195\n",
      "         63       0.98      0.91      0.94       178\n",
      "         64       1.00      0.94      0.97       159\n",
      "\n",
      "avg / total       0.94      0.93      0.94     29557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, X_train_stacked['pred_description']))\n",
    "print(metrics.classification_report(y_train, X_train_stacked['pred_html2text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.28      0.29       292\n",
      "          1       0.64      0.71      0.68       211\n",
      "          2       0.50      0.27      0.35        74\n",
      "          3       0.44      0.59      0.50       582\n",
      "          4       0.54      0.49      0.51       106\n",
      "          5       0.57      0.65      0.61        99\n",
      "          6       0.47      0.42      0.44       110\n",
      "          7       0.60      0.50      0.54        74\n",
      "          8       0.72      0.66      0.69       115\n",
      "          9       0.71      0.78      0.74        65\n",
      "         10       0.52      0.29      0.38        51\n",
      "         11       0.55      0.66      0.60       275\n",
      "         12       0.69      0.79      0.74       116\n",
      "         13       0.58      0.59      0.58       110\n",
      "         14       0.44      0.47      0.45       146\n",
      "         15       0.34      0.26      0.29       189\n",
      "         16       0.52      0.63      0.57       483\n",
      "         17       0.51      0.46      0.49        78\n",
      "         18       0.33      0.12      0.17        43\n",
      "         19       0.52      0.45      0.48        71\n",
      "         20       0.43      0.40      0.41       118\n",
      "         21       0.60      0.81      0.69        63\n",
      "         22       0.76      0.78      0.77       236\n",
      "         23       0.43      0.49      0.46       268\n",
      "         24       0.64      0.53      0.58        53\n",
      "         25       0.58      0.54      0.56       180\n",
      "         26       0.42      0.39      0.41        82\n",
      "         27       0.60      0.50      0.55        48\n",
      "         28       0.60      0.44      0.51        57\n",
      "         29       0.39      0.36      0.37       166\n",
      "         30       0.58      0.27      0.37        41\n",
      "         31       0.56      0.64      0.60       127\n",
      "         32       0.37      0.27      0.31        67\n",
      "         33       0.57      0.66      0.61       148\n",
      "         34       0.63      0.60      0.61       107\n",
      "         35       0.49      0.41      0.45       109\n",
      "         36       0.74      0.78      0.76       131\n",
      "         37       0.57      0.51      0.54        95\n",
      "         38       0.52      0.43      0.47        69\n",
      "         39       0.61      0.63      0.62        81\n",
      "         40       0.68      0.46      0.55        50\n",
      "         41       0.72      0.74      0.73       103\n",
      "         42       0.64      0.62      0.63       109\n",
      "         43       0.55      0.65      0.60       167\n",
      "         44       0.48      0.32      0.38        44\n",
      "         45       0.51      0.40      0.45        92\n",
      "         46       0.54      0.52      0.53        42\n",
      "         47       0.66      0.51      0.57        45\n",
      "         48       0.47      0.53      0.50       175\n",
      "         49       0.59      0.46      0.52        50\n",
      "         50       0.62      0.70      0.66       109\n",
      "         51       0.46      0.37      0.41        99\n",
      "         52       0.56      0.51      0.53        73\n",
      "         53       0.41      0.30      0.35        60\n",
      "         54       0.36      0.26      0.30        66\n",
      "         55       0.29      0.15      0.20        53\n",
      "         56       0.65      0.57      0.61        96\n",
      "         57       0.76      0.65      0.70        54\n",
      "         58       0.50      0.16      0.25        43\n",
      "         59       0.42      0.34      0.38        73\n",
      "         60       0.48      0.48      0.48        62\n",
      "         61       0.40      0.38      0.39        55\n",
      "         62       0.51      0.41      0.45        49\n",
      "         63       0.71      0.67      0.69        45\n",
      "         64       0.63      0.47      0.54        40\n",
      "\n",
      "avg / total       0.53      0.53      0.52      7390\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.22      0.21       292\n",
      "          1       0.52      0.53      0.53       211\n",
      "          2       0.38      0.20      0.26        74\n",
      "          3       0.27      0.53      0.35       582\n",
      "          4       0.38      0.27      0.32       106\n",
      "          5       0.55      0.54      0.54        99\n",
      "          6       0.41      0.27      0.33       110\n",
      "          7       0.45      0.49      0.47        74\n",
      "          8       0.58      0.49      0.53       115\n",
      "          9       0.63      0.62      0.62        65\n",
      "         10       0.24      0.10      0.14        51\n",
      "         11       0.38      0.51      0.43       275\n",
      "         12       0.60      0.65      0.62       116\n",
      "         13       0.39      0.36      0.38       110\n",
      "         14       0.26      0.29      0.27       146\n",
      "         15       0.20      0.12      0.15       189\n",
      "         16       0.32      0.52      0.40       483\n",
      "         17       0.40      0.27      0.32        78\n",
      "         18       0.00      0.00      0.00        43\n",
      "         19       0.34      0.30      0.32        71\n",
      "         20       0.29      0.24      0.26       118\n",
      "         21       0.49      0.44      0.47        63\n",
      "         22       0.61      0.67      0.64       236\n",
      "         23       0.31      0.37      0.34       268\n",
      "         24       0.45      0.38      0.41        53\n",
      "         25       0.37      0.38      0.37       180\n",
      "         26       0.39      0.27      0.32        82\n",
      "         27       0.59      0.40      0.48        48\n",
      "         28       0.50      0.30      0.37        57\n",
      "         29       0.23      0.19      0.21       166\n",
      "         30       0.17      0.07      0.10        41\n",
      "         31       0.44      0.45      0.45       127\n",
      "         32       0.43      0.24      0.31        67\n",
      "         33       0.36      0.40      0.38       148\n",
      "         34       0.42      0.27      0.33       107\n",
      "         35       0.40      0.34      0.37       109\n",
      "         36       0.68      0.62      0.65       131\n",
      "         37       0.48      0.41      0.44        95\n",
      "         38       0.33      0.23      0.27        69\n",
      "         39       0.23      0.19      0.21        81\n",
      "         40       0.50      0.16      0.24        50\n",
      "         41       0.80      0.59      0.68       103\n",
      "         42       0.57      0.48      0.52       109\n",
      "         43       0.46      0.40      0.43       167\n",
      "         44       0.39      0.16      0.23        44\n",
      "         45       0.28      0.20      0.23        92\n",
      "         46       0.42      0.19      0.26        42\n",
      "         47       0.46      0.27      0.34        45\n",
      "         48       0.37      0.45      0.40       175\n",
      "         49       0.38      0.36      0.37        50\n",
      "         50       0.51      0.46      0.48       109\n",
      "         51       0.28      0.20      0.23        99\n",
      "         52       0.43      0.36      0.39        73\n",
      "         53       0.23      0.13      0.17        60\n",
      "         54       0.25      0.11      0.15        66\n",
      "         55       0.20      0.06      0.09        53\n",
      "         56       0.45      0.32      0.38        96\n",
      "         57       0.41      0.35      0.38        54\n",
      "         58       0.15      0.07      0.10        43\n",
      "         59       0.18      0.10      0.12        73\n",
      "         60       0.35      0.34      0.34        62\n",
      "         61       0.39      0.16      0.23        55\n",
      "         62       0.41      0.22      0.29        49\n",
      "         63       0.54      0.44      0.49        45\n",
      "         64       0.50      0.30      0.37        40\n",
      "\n",
      "avg / total       0.38      0.38      0.37      7390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, X_test_stacked['pred_description']))\n",
    "print(metrics.classification_report(y_test, X_test_stacked['pred_html2text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
